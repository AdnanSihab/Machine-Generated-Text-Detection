{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7000097,"sourceType":"datasetVersion","datasetId":4024039},{"sourceId":7011051,"sourceType":"datasetVersion","datasetId":4030966}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","import pandas as pd"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-22T11:00:56.478838Z","iopub.execute_input":"2023-11-22T11:00:56.479324Z","iopub.status.idle":"2023-11-22T11:01:01.159181Z","shell.execute_reply.started":"2023-11-22T11:00:56.479281Z","shell.execute_reply":"2023-11-22T11:01:01.158370Z"},"trusted":true,"id":"73cN8q6kFaUe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_imdb_data(data_file):\n","    df = data_file\n","    texts = df['text'].tolist()\n","    labels = df['label'].tolist()\n","    return texts, labels"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:01:09.939451Z","iopub.execute_input":"2023-11-22T11:01:09.940275Z","iopub.status.idle":"2023-11-22T11:01:09.945152Z","shell.execute_reply.started":"2023-11-22T11:01:09.940242Z","shell.execute_reply":"2023-11-22T11:01:09.944201Z"},"trusted":true,"id":"LRSGxZLKFaUf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"/kaggle/input/semval/output.csv\")"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:01:13.692380Z","iopub.execute_input":"2023-11-22T11:01:13.692768Z","iopub.status.idle":"2023-11-22T11:01:21.305912Z","shell.execute_reply.started":"2023-11-22T11:01:13.692735Z","shell.execute_reply":"2023-11-22T11:01:21.305106Z"},"trusted":true,"id":"4obB9s4mFaUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_counts = df[\"label\"].value_counts()\n","\n","=\n","print(label_counts)"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:01:21.307606Z","iopub.execute_input":"2023-11-22T11:01:21.307908Z","iopub.status.idle":"2023-11-22T11:01:21.327058Z","shell.execute_reply.started":"2023-11-22T11:01:21.307883Z","shell.execute_reply":"2023-11-22T11:01:21.326130Z"},"trusted":true,"id":"KS5FNd8YFaUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_file = df\n","texts, labels = load_imdb_data(data_file)"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:01:21.328289Z","iopub.execute_input":"2023-11-22T11:01:21.328613Z","iopub.status.idle":"2023-11-22T11:01:21.341296Z","shell.execute_reply.started":"2023-11-22T11:01:21.328581Z","shell.execute_reply":"2023-11-22T11:01:21.340437Z"},"trusted":true,"id":"peaPNbz-FaUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TextClassificationDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","    def __len__(self):\n","        return len(self.texts)\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n","        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:01:21.343301Z","iopub.execute_input":"2023-11-22T11:01:21.343758Z","iopub.status.idle":"2023-11-22T11:01:21.353560Z","shell.execute_reply.started":"2023-11-22T11:01:21.343728Z","shell.execute_reply":"2023-11-22T11:01:21.352570Z"},"trusted":true,"id":"apVtndTSFaUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import RobertaModel\n","\n","class RoBERTaClassifier(nn.Module):\n","    def __init__(self, roberta_model_name, num_classes):\n","        super(RoBERTaClassifier, self).__init__()\n","        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n","        self.dropout = nn.Dropout(0.1)\n","        self.fc = nn.Linear(self.roberta.config.hidden_size, num_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.pooler_output\n","        x = self.dropout(pooled_output)\n","        logits = self.fc(x)\n","        return logits"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:01:21.354751Z","iopub.execute_input":"2023-11-22T11:01:21.355091Z","iopub.status.idle":"2023-11-22T11:01:21.370050Z","shell.execute_reply.started":"2023-11-22T11:01:21.355058Z","shell.execute_reply":"2023-11-22T11:01:21.369214Z"},"trusted":true,"id":"TzfNlYbSFaUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, data_loader, optimizer, scheduler, device):\n","    model.train()\n","    for batch in data_loader:\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['label'].to(device)\n","\n","        # Forward pass\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits  # Access logits from the output\n","\n","        # Compute loss\n","        loss = nn.CrossEntropyLoss()(logits, labels)\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:01:21.371126Z","iopub.execute_input":"2023-11-22T11:01:21.371394Z","iopub.status.idle":"2023-11-22T11:01:21.383848Z","shell.execute_reply.started":"2023-11-22T11:01:21.371364Z","shell.execute_reply":"2023-11-22T11:01:21.383026Z"},"trusted":true,"id":"4ADLMCdPFaUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, data_loader, device):\n","    model.eval()\n","    predictions = []\n","    actual_labels = []\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['label'].to(device)\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","            # Extract logits from the outputs\n","            logits = outputs.logits\n","\n","            _, preds = torch.max(logits, dim=1)\n","            predictions.extend(preds.cpu().tolist())\n","            actual_labels.extend(labels.cpu().tolist())\n","    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:01:21.384856Z","iopub.execute_input":"2023-11-22T11:01:21.385155Z","iopub.status.idle":"2023-11-22T11:01:21.395580Z","shell.execute_reply.started":"2023-11-22T11:01:21.385126Z","shell.execute_reply":"2023-11-22T11:01:21.394817Z"},"trusted":true,"id":"xPiCcsCrFaUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_sentiment(text, model, tokenizer, device, max_length=128):\n","    model.eval()\n","    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n","    input_ids = encoding['input_ids'].to(device)\n","    attention_mask = encoding['attention_mask'].to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits  # Access logits from the output\n","        _, preds = torch.max(logits, dim=1)\n","\n","        return \"Machine\" if preds.item() == 1 else \"Human\"\n"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:36:10.332859Z","iopub.execute_input":"2023-11-22T11:36:10.333783Z","iopub.status.idle":"2023-11-22T11:36:10.340398Z","shell.execute_reply.started":"2023-11-22T11:36:10.333746Z","shell.execute_reply":"2023-11-22T11:36:10.339354Z"},"trusted":true,"id":"5zpK8Ob3FaUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["roberta_model_name = \"roberta-base\"\n","num_classes = 2\n","max_length = 128\n","batch_size = 16\n","learning_rate = 1e-5\n","num_epochs = 1"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:01:21.412434Z","iopub.execute_input":"2023-11-22T11:01:21.412714Z","iopub.status.idle":"2023-11-22T11:01:21.422523Z","shell.execute_reply.started":"2023-11-22T11:01:21.412690Z","shell.execute_reply":"2023-11-22T11:01:21.421611Z"},"trusted":true,"id":"1P-D2NaFFaUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, shuffle = True)\n","\n"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:01:21.425400Z","iopub.execute_input":"2023-11-22T11:01:21.425739Z","iopub.status.idle":"2023-11-22T11:01:21.497878Z","shell.execute_reply.started":"2023-11-22T11:01:21.425707Z","shell.execute_reply":"2023-11-22T11:01:21.496956Z"},"trusted":true,"id":"4DMI93IbFaUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["zero = 0\n","one = 0\n","for i,j in zip(train_texts,train_labels):\n","    if j==0:\n","        zero+=1\n","    else:\n","        one+=1\n","\n","print(zero)\n","print(one)"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:01:27.651924Z","iopub.execute_input":"2023-11-22T11:01:27.652275Z","iopub.status.idle":"2023-11-22T11:01:27.698815Z","shell.execute_reply.started":"2023-11-22T11:01:27.652249Z","shell.execute_reply":"2023-11-22T11:01:27.698022Z"},"trusted":true,"id":"XvqXYDX2FaUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = RobertaTokenizer.from_pretrained(roberta_model_name)\n","train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n","val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:01:30.071469Z","iopub.execute_input":"2023-11-22T11:01:30.072350Z","iopub.status.idle":"2023-11-22T11:01:30.958580Z","shell.execute_reply.started":"2023-11-22T11:01:30.072315Z","shell.execute_reply":"2023-11-22T11:01:30.957799Z"},"trusted":true,"id":"ofaG2TBkFaUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = RobertaForSequenceClassification.from_pretrained(roberta_model_name, num_labels=num_classes).to(device)"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:01:32.664067Z","iopub.execute_input":"2023-11-22T11:01:32.664417Z","iopub.status.idle":"2023-11-22T11:01:39.940619Z","shell.execute_reply.started":"2023-11-22T11:01:32.664390Z","shell.execute_reply":"2023-11-22T11:01:39.939612Z"},"trusted":true,"id":"pTh-MBRMFaUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = AdamW(model.parameters(), lr=learning_rate, correct_bias=False, no_deprecation_warning=True)\n","total_steps = len(train_dataloader) * num_epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:01:44.704505Z","iopub.execute_input":"2023-11-22T11:01:44.705444Z","iopub.status.idle":"2023-11-22T11:01:44.713712Z","shell.execute_reply.started":"2023-11-22T11:01:44.705404Z","shell.execute_reply":"2023-11-22T11:01:44.712893Z"},"trusted":true,"id":"YhimgtE8FaUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","    train(model, train_dataloader, optimizer, scheduler, device)\n","    print(\"done training\")\n","    accuracy, report = evaluate(model, val_dataloader, device)\n","    print(f\"Validation Accuracy: {accuracy:.4f}\")\n","    print(report)"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:01:46.684401Z","iopub.execute_input":"2023-11-22T11:01:46.684745Z","iopub.status.idle":"2023-11-22T11:32:45.318386Z","shell.execute_reply.started":"2023-11-22T11:01:46.684719Z","shell.execute_reply":"2023-11-22T11:32:45.317450Z"},"trusted":true,"id":"G-6PNSXLFaUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"robert_classifier.pth\")"],"metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:34:09.522750Z","iopub.execute_input":"2023-11-22T11:34:09.523149Z","iopub.status.idle":"2023-11-22T11:34:10.240785Z","shell.execute_reply.started":"2023-11-22T11:34:09.523120Z","shell.execute_reply":"2023-11-22T11:34:10.239900Z"},"trusted":true,"id":"5jshbryJFaUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Defining a function to load data: The load_imdb_data function is defined to load your data from a given file. It reads the ‘text’ and ‘label’ columns from the DataFrame and returns them as lists.\n","\n","# Loading and processing the data: You’re loading a CSV file into a pandas DataFrame. Then, you’re creating a new DataFrame by concatenating the first 50000 and the last 50000 rows of the original DataFrame. You’re also displaying the last 50 rows of the new DataFrame. After that, you’re printing the value counts of the ‘label’ column in the DataFrame. Finally, you’re loading the texts and labels from the DataFrame using the load_imdb_data function.\n","\n","# Creating a custom Dataset class: You’re defining a custom Dataset class TextClassificationDataset for your text classification task. This class takes in texts, labels, a tokenizer, and a max_length as inputs. It tokenizes the texts and returns the input_ids, attention_mask, and label for each text.\n","\n","# Creating a BERT Classifier: You’re defining a BERT Classifier BERTClassifier which is a subclass of nn.Module. This classifier uses a pre-trained BERT model and a linear layer for classification. The forward method of this class takes in input_ids and attention_mask, and returns the logits.\n","\n","# Defining training and evaluation functions: You’re defining a train function to train your model and an evaluate function to evaluate your model’s performance. The train function takes in a model, a data loader, an optimizer, a scheduler, and a device as inputs, and trains the model. The evaluate function takes in a model, a data loader, and a device as inputs, and returns the accuracy score and the classification report.\n","\n","# Defining a function for sentiment prediction: You’re defining a predict_sentiment function that takes a text, a model, a tokenizer, a device, and a max_length as inputs, and returns the predicted sentiment (“Machine” or “Human”) for the text.\n","\n","# Setting parameters: You’re setting some parameters for your task, including the BERT model name, the number of classes, the max_length for tokenization, the batch size, the number of epochs, and the learning rate.\n","\n","# Splitting the data into training and validation sets: You’re splitting your texts and labels into training and validation sets using the train_test_split function from sklearn."],"metadata":{"trusted":true,"id":"sBcUysdWFaUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Counting the labels: You’re counting the number of instances for each label in your training data.\n","\n","# Tokenizing the texts: You’re initializing a tokenizer from the pre-trained BERT model and using it to tokenize the texts in your training and validation sets.\n","\n","# Creating DataLoaders: You’re creating PyTorch DataLoaders for your training and validation datasets. These DataLoaders will be used to feed data into your model during training and evaluation.\n","\n","# Setting up the device: You’re setting up the device (GPU if available, otherwise CPU) for training your model.\n","\n","# Initializing the model: You’re initializing your BERT Classifier and moving it to the device.\n","\n","# Setting up the optimizer and scheduler: You’re setting up the AdamW optimizer with your model’s parameters and the learning rate. You’re also setting up a learning rate scheduler.\n","\n","# Training and evaluating the model: You’re training your model for a certain number of epochs. After each epoch, you’re evaluating your model on the validation set and printing the validation accuracy and the classification report.\n","\n","# Saving the model: You’re saving the state dictionary of your model to a file.\n","\n","# Predicting the sentiment: You’re asking the user to enter a text, predicting the sentiment of this text using your trained model, and printing the predicted sentiment."],"metadata":{"trusted":true,"id":"5AU1OXLEFaUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"trusted":true,"id":"Cd5_-0tHFaUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mIs8ldEGFaUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o3vwsjRFFaUi"},"execution_count":null,"outputs":[]}]}